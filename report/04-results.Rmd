# Results

## Existing scenario evaluation
UTA, in partnership with VIA, ran a pilot program of microtransit service in south Salt Lake County from December 2019--November 2020. We also created a scenario in BEAM to model this pilot program. In order to assess the performance of our BEAM simulations when compared with real-world observed data, we compared the results of the two on several metrics (Table \@ref(tab:existing-comparison)).

```{r existing-comparison}
tar_load(existing_comparison)

existing_comparison %>%
  kbl(booktabs = TRUE, digits = c(0,0,2,1),
      caption = "Comparison of Observed Data with 'Existing' BEAM Scenario") %>%
  kable_styling()
```

At first glance, these results seem to be quite good, as the simulated scenario reports similar ridership and wait time metrics to the real-world data. However, it is important to consider that our model scenario uses a 20% population sample, and the observed data reflects the entire population. The model ODT fleet, on the other hand, was not scaled down to match the population sample size, and so represents the full fleet available in the real-world pilot program.

The fact that the predicted ridership is so similar to the actual, measured ridership is nevertheless encouraging. This suggests that the predicted ridership in the other modeled scenarios is a somewhat realistic forecast, albeit a rough approximation.

There is also a significant discrepancy in the utilization measurements. Because of the way utilization is calculated (passengers per hour per vehicle), the vehicle operating hours can greatly affect this metric. In our model scenario, all 12 of the ODT vehicles are operational nearly all day, and it is likely that in off-peak hours the actual pilot program had fewer vehicles available. If operating hours are taken to be only time that the ODT vehicles are in motion, then our modeled utilization becomes **TODO:** calculate this number.


## Candidate scenario comparison
We then ran each of our scenarios as described in section \@ref(scenario-configuration), and compared several metrics. These metrics include total ridership, utilization, and wait time, as in the previous comparison, but also include ***Other metrics***. These comparisons are given in Table \@ref(tab:scenario-compare), and a plot of the wait times in Figure \@ref(fig:wait-time-compare).

```{r scenario-compare}
tar_read(scenario_comparison) %>% 
  kbl(booktabs = TRUE, digits = c(0,0,0,2,1),
      caption = "Comparison Across BEAM Scenarios") %>% 
  kable_styling()
```

It is clear that in the scenarios with more microtransit vehicles available, ridership greatly increases. Interestingly, utilization does not vary much at all, which implies that at least in these specific scenarios there is a roughly linear relationship between number of vehicles and number of riders. This relationship can also be seen in the ratios of passengers to fleet size, as all of the vehicle operating hours are identical. In fact, fitting a linear model to this data gives a predicted increase of about 28 passengers per ODT vehicle. **TODO:** calculate this number in R.

```{r wait-time-compare}
#| fig.cap="Comparison of ODT wait times in each scenario."

tar_read(wait_time_list) %>% 
 ggplot() +
  geom_violin(aes(x = wait_time, y = Scenario, fill = Scenario)) +
  theme_pander() +
  easy_remove_legend("fill") +
  labs(x = "Time between ODT request and fulfillment",
       y = "Scenario")
```

Wait times also hardly are affected in the various scenarios. Not only are the average wait times nearly identical between scenarios (as shown in Table \@ref(tab:scenario-compare)), the distribution of wait times is essentially the same as well. It is important to note though that these wait times represent only _fulfilled_ ODT requests. In BEAM, when an agent makes an ODT request they may afterward re-plan and choose a different mode. Often this decision is largely based on the projected wait time for the ODT vehicle, so a request that returns a long wait time will often result in a change to mode choice. Table \@ref(tab:odt-fulfillment) compares the proportion of fulfilled requests in each scenario. **TODO:** create this table.

```{r odt-fulfillment}
tibble("temp") %>% kbl(caption = "temp")
# tar_read(rh_fulfillment_comparison) %>% 
#   kbl(booktabs = TRUE, digits = c(0,0,0,0,2), #scenario, #req, #ful, #not, proportion
#       caption = "Comparison of Fulfilled and Unfulfilled ODT Requests") %>% 
#   kable_styling()
```


## Summary

More Text
